{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"green_tripdata_2015-09.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 part 1:\n",
    "Based on http://www.nyc.gov/html/tlc/html/passenger/taxicab_rate.shtml, the initial charge is $2.50.\n",
    "So all the records with Fare_amount < 2.5 are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Tip percentage\n",
      "count    1.475973e+06\n",
      "mean     9.288832e-02\n",
      "std      1.750175e-01\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      2.133333e-01\n",
      "max      3.750000e+01\n",
      "Name: tip%, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_tip = df.copy()\n",
    "df_tip =df_tip[df_tip.loc[:, \"Fare_amount\"]>2.5]\n",
    "df_tip.loc[:, \"tip%\"] = df_tip[\"Tip_amount\"].div(df_tip.Fare_amount)\n",
    "print \"Summary: Tip percentage\\n\",df_tip[\"tip%\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Question 4 part 2:\n",
    "Building a predictive model for tip as a percentage of the total fare.\n",
    "\n",
    "I used two sources for building the prediction model:\n",
    "http://www.skytree.net/2016/07/14/nyc-taxi-blog-series-2-of-2-dataset-analysis/\n",
    "also I got some ideas from this paper:\n",
    "https://cseweb.ucsd.edu/~jmcauley/cse190/reports/sp15/050.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Step for building a model is data cleaning.\n",
    "Data cleaning consists:\n",
    "1. removing (Ehail_fee), since since 99% are Null\n",
    "2. replacing missing and invalid values with the most frequent values categorical variables\n",
    "3. replacing invalid values with the median for continuous variables\n",
    "4. replacing negative values for varibales such as fare_amount, tip_amount... with their absolute values \n",
    "5. converting datetime varibales to their correct format\n",
    "6. removing those transaction that \"payment_type\" is not with Credit Card\n",
    "\n",
    "##### Secod Step is feature engineering:\n",
    "1. Add 9 new derived features that are created from pickup and dropoff timestamps, trip distance.\n",
    "2. Also adding trip_percentage feature\n",
    "\n",
    "##### Third Step is building a model:\n",
    "1. I use random forest regressor for building a model(unfortunately due to time limit, I was not able to test other models and compare different models )\n",
    "2. In order of optimization I optimized the number of trees, since they have significant impact on model accuracy.\n",
    "3. And number of trees = 80 is the optimized one for a model with important features.\n",
    "4. I build models with sample size of 100,000 and the models are crossed validate with 3 folds cross-validation\n",
    "5. The builded models show that important featues are: \"Fare_amount\", \"Total_amount\", \"Tip_amount\" and \"trip_duration\", \"speed_mph\"\n",
    "6. Finally, I used all the samples for buiding the prediction model but since it has a big size I used 100,000 samples for submission. Also, I used only important features and n_estimator = 80\n",
    "7. Created model saved with pickle in a file named: 'pre_tip.pkl'\n",
    "8. For making prediction run tip_prediction.make_prediction(test_data)\n",
    "9. As conclusion, I think by having some information about passenger can affect accuracy as well.\n",
    "10. Average mean_sqyared_error on different test sample set is: 20.18\n",
    "#### Running the prediction model\n",
    "A python file (tip_prediction.py) is provided. For making prediction use tip_prediction.predict(data),  where data is any 2015 raw dataframe from given url: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *********************** first step is data cleaning ***********************\n",
    "def data_cleanining(df_new):\n",
    "    \n",
    "    # 1. remove \"Ehail_fee\" column since 99% are Null(missing values)\n",
    "    del df_new[\"Ehail_fee\"]\n",
    "\n",
    "    # 2. replace missing value in \"Trip_type\" by most frequent value which is 1\n",
    "    mfv_t = df_new['Trip_type '].value_counts().idxmax()\n",
    "    df_new['Trip_type '].replace(np.NaN, 1,inplace=True)\n",
    "\n",
    "    # 3.replace invalid value in \"RateCodeID\" which is 99 by most frequent value which is 1\n",
    "    # Rate Code values include: 1, 2, 3, 4, 5, 6\n",
    "    mfv_r = df_new['RateCodeID'].value_counts().idxmax()\n",
    "    df_new.RateCodeID[~((df_new.RateCodeID>=1) & (df_new.RateCodeID<=6))] = mfv_r\n",
    "\n",
    "    # 4. replace invalid values in \"Extra\" by most frequent value which is 0\n",
    "    # Extra can have only these values: 0, 0.50 and 1\n",
    "    mfv_e = df_new['Extra'].value_counts().idxmax()\n",
    "    df_new.Extra[~((df_new.Extra==0.5) |(df_new.Extra==1) | (df_new.RateCodeID==0.5))] = mfv_e\n",
    "\n",
    "    # 5. replace invalid values in \"Passenger_count\" which is 0 by most frequent value which is 1\n",
    "    mfv_p = df_new['Passenger_count'].value_counts().idxmax()\n",
    "    df_new.Passenger_count[~((df_new.Passenger_count> 0))] = mfv_p\n",
    "\n",
    "    # 6. replace values in \"Trip_distance\" <=0 by median\n",
    "    med_td = df_new['Trip_distance'].median()\n",
    "    df_new.Trip_distance[(df_new.Trip_distance <= 0)] = med_td\n",
    "\n",
    "    # 7. Negative values found and replaced by their abs\n",
    "    df_new.Fare_amount = df_new.Fare_amount.abs()\n",
    "    df_new.Tip_amount = df_new.Tip_amount.abs()\n",
    "    df_new.Total_amount = df_new.Total_amount.abs()\n",
    "    df_new.Tolls_amount = df_new.Tolls_amount.abs()\n",
    "    df_new.improvement_surcharge = df_new.improvement_surcharge.abs()\n",
    "    df_new.MTA_tax = df_new.MTA_tax.abs()\n",
    "\n",
    "    # 8. replace values in \"Fare_amount\" <  2.5 by median\n",
    "    med_fm= df_new['Fare_amount'].median()\n",
    "    df_new.Fare_amount[(df_new.Fare_amount < 2.5)] = med_fm\n",
    "\n",
    "    # 9. replace values in \"Total_amount\" <  2.5 by median\n",
    "    med_tm= df_new['Total_amount'].median()\n",
    "    df_new.Total_amount[(df_new.Total_amount < 2.5)] = med_tm\n",
    "\n",
    "    # 10. Assuming 0 = N and 1 = Y, change N to 0 and Y to 1\n",
    "    df_new['Store_and_fwd_flag'].replace('Y', 1, inplace=True)\n",
    "    df_new['Store_and_fwd_flag'].replace('N', 0, inplace=True)\n",
    "\n",
    "    # 11. converting the \"lpep_pickup_datetime\" and \"Lpep_dropoff_datetime\" to DateTime series\n",
    "    df_new.loc[:, \"lpep_pickup_datetime\"] = pd.to_datetime(df_new.loc[:, \"lpep_pickup_datetime\"])\n",
    "    df_new.loc[:, \"Lpep_dropoff_datetime\"] = pd.to_datetime(df_new.loc[:, \"Lpep_dropoff_datetime\"])\n",
    "\n",
    "    # 12. remove those rows that \"payment_type\" is not with Credit Card (1)\n",
    "    df_new = df_new[df_new.Payment_type == 1]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# *********************** Second step is feature engineering ***********************\n",
    "def data_engin(df_new):\n",
    "    \n",
    "    # 1. creating time varibales from \"lpep_pickup_datetime\" and \"Lpep_dropoff_datetime\"\n",
    "\n",
    "    # int [0-23], hour the day a transaction was done\n",
    "    df_new.loc[:, \"hour_p\"] = df_new.loc[:, \"lpep_pickup_datetime\"].dt.hour\n",
    "    df_new.loc[:, \"hour_d\"] = df_new.loc[:, \"Lpep_dropoff_datetime\"].dt.hour\n",
    "    # int [0-6], day of the week a transaction was done\n",
    "    df_new.loc[:, \"weekday_p\"] = df_new.loc[:, \"lpep_pickup_datetime\"].dt.weekday\n",
    "    df_new.loc[:, \"weekday_d\"] = df_new.loc[:, \"Lpep_dropoff_datetime\"].dt.weekday\n",
    "    #Month_day: int [0-30], day of the month a transaction was done\n",
    "    df_new.loc[:, \"monthday_p\"] = df_new.loc[:, \"lpep_pickup_datetime\"].dt.day\n",
    "    df_new.loc[:, \"monthday_d\"] = df_new.loc[:, \"Lpep_dropoff_datetime\"].dt.day\n",
    "    # Trip Duration in minutes\n",
    "    df_new.loc[:,'trip_duration'] = df_new.loc[:, \"Lpep_dropoff_datetime\"] - df_new.loc[:, \"lpep_pickup_datetime\"]\n",
    "    df_new.loc[:,'trip_duration'] = df_new.loc[:,'trip_duration'].dt.total_seconds()/60\n",
    "    # removing \"lpep_pickup_datetime\" and \"Lpep_dropoff_datetime\"\n",
    "    del df_new[\"lpep_pickup_datetime\"]\n",
    "    del df_new[\"Lpep_dropoff_datetime\"]\n",
    "\n",
    "\n",
    "    # 2. speed variable\n",
    "    df_new.loc[:,'speed_mph'] = df_new.Trip_distance/(df_new.trip_duration/60)\n",
    "    # remove transactions that speed in not in valid range\n",
    "    df_new = df_new[((df_new.speed_mph>0) & (df_new.speed_mph<=240))]\n",
    "    # 3. location variable\n",
    "\n",
    "    # 4.  create tip percentage variable\n",
    "    df_new['tip_percentage'] = 100*df_new.Tip_amount/df_new.Fare_amount\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function finds the optimized number of estimators of the model\n",
    "\n",
    "def find_optimized_number_est(data_sample):\n",
    "    \"\"\"\n",
    "        this function finds the optimized number of estimators of the model\n",
    "        data_sample: pandas data frame\n",
    "        return integer as optimized # of estimator\n",
    "    \"\"\"\n",
    "    data = data_sample.ix[:, data_sample.columns != 'tip_percentage']\n",
    "    target = data_sample['tip_percentage']\n",
    "    # randomly hold out 20% of the data as test set\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    data, target, test_size=.20, random_state=0)\n",
    "    mse_final = []\n",
    "    for j in range(50, 220, 20):\n",
    "        rf = RandomForestRegressor(n_estimators=j)\n",
    "        rf.fit(X_train, y_train)\n",
    "        predicted = rf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, predicted)\n",
    "        mse_final.append((j, mse))\n",
    "    min_mse = min(mse_final, key = lambda t: t[1])\n",
    "    print(min_mse[0])\n",
    "    return min_mse[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction_with_RFR(df_sample, n_opt, k):\n",
    "    \n",
    "    \"\"\"\n",
    "        Build and Evaluate the model using k-fold cross-validation\n",
    "        param data: pandas dataframe\n",
    "        param n_opt: optimised number of estimator\n",
    "        param k: k_fold CV\n",
    "        return Mean Squared error with 3 fold cross_validation\n",
    "\n",
    "    \"\"\"\n",
    "    data = df_sample.ix[:, df_sample.columns != 'tip_percentage']\n",
    "    target = df_sample['tip_percentage']\n",
    "    mse_avg = []\n",
    "    for i in range(k): # repeat the procedure k times to get more precise results\n",
    "        \n",
    "        # for each iteration, randomly hold out 20% of the data as test set\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "        data, target, test_size=.20, random_state=0)\n",
    "        rf = RandomForestRegressor(n_estimators= n_opt)\n",
    "        rf.fit(X_train, y_train)\n",
    "    #         print(rf.feature_importances_)\n",
    "        predicted = rf.predict(X_test)\n",
    "        print(\"score\", rf.score(X_test, y_test))\n",
    "        mse = mean_squared_error(y_test, predicted)\n",
    "        print(\"Mean Squared error\", mse)\n",
    "        mse_avg.append(mse)\n",
    "    return np.mean(mse_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_optimized_prediction_model(df_sample, n_opt):\n",
    "    \"\"\"\n",
    "    build the prediction model with all the samples and save \n",
    "    the model with pickle \n",
    "    \"\"\"\n",
    "#     print(len(df_sample.columns))\n",
    "    data = df_sample.ix[:, df_sample.columns != 'tip_percentage']\n",
    "    target = df_sample['tip_percentage']\n",
    "    rf = RandomForestRegressor(n_estimators= n_opt)\n",
    "    rf.fit(data, target)\n",
    "#     print(rf.feature_importances_)\n",
    "    # save the model to disk\n",
    "    filename = 'pre_tip.pkl'\n",
    "    pickle.dump(rf, open(filename, 'wb'))\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_tip(test):\n",
    "    \"\"\"\n",
    "        this function loads the model from disk and predict the tip for test data\n",
    "        param test: pandas data frame that is cleaned and feature engineering is done\n",
    "    \"\"\"\n",
    "    # load the model from disk\n",
    "    filename = 'pre_tip.pkl'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    X_test = test.ix[:, test.columns != 'tip_percentage']\n",
    "    Y_test = test['tip_percentage']\n",
    "    predicted = loaded_model.predict(X_test)\n",
    "    print(\"score\", loaded_model.score(X_test, Y_test ))\n",
    "    mse = mean_squared_error(Y_test , predicted)\n",
    "    print(\"Mean Squared error\", mse)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimization and validation\n",
    "In this section I found optimized number of estimators, which is 150. (for a model with all the features) \n",
    "\n",
    "I build a model with sample size of 100,000 and evaulate the model using k-fold cross-validation (k=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data cleaning\n",
      "feature engineering\n",
      "('score', 0.95884104640622525)\n",
      "('Mean Squared error', 37.094230075193245)\n",
      "('score', 0.96388886669833063)\n",
      "('Mean Squared error', 32.544915990543892)\n",
      "('score', 0.95743067148872574)\n",
      "('Mean Squared error', 38.365320982857014)\n",
      "('Mean Squared error with', 3, 'fold cross_validation', 36.00148901619805)\n"
     ]
    }
   ],
   "source": [
    "df_new = df.copy()\n",
    "print \"data cleaning\"\n",
    "df_new = data_cleanining(df_new)\n",
    "print \"feature engineering\"\n",
    "df_new = data_engin(df_new)\n",
    "# # find optimized n_estimators on sample data of size 100,000\n",
    "df_sample = df_new.loc[np.random.choice(df_new.index,size=100000,replace=False)]\n",
    "n_opt = find_optimized_number_est(df_sample)\n",
    "n_opt = 150\n",
    "# evaluating the model with 3 folds cross-validation\n",
    "k = 3 # number of CV\n",
    "# Sample size for training and optimization was chosen as 100,000 with 3 folds cross-validation\n",
    "df_sample = df_new.loc[np.random.choice(df_new.index,size=100000,replace=False)]\n",
    "# Build and Evaluate the model using k-fold cross-validation\n",
    "mse_final = evaluate_prediction_with_RFR(df_sample, n_opt, k)\n",
    "print (\"Mean Squared error with\", k, \"fold cross_validation\" , mse_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build and save model\n",
    "This section is for saving the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data cleaning\n",
      "feature engineering\n",
      "[ 0.38738767  0.05085928  0.53513401  0.00736876  0.01925027]\n"
     ]
    }
   ],
   "source": [
    "# build and save model for later use\n",
    "df_new = df.copy()\n",
    "print \"data cleaning\"\n",
    "df_new = data_cleanining(df_new)\n",
    "print \"feature engineering\"\n",
    "df_new = data_engin(df_new)\n",
    "df_sample = df_new.loc[np.random.choice(df_new.index,size=100000,replace=False)]\n",
    "# # df_sample = df_new # build the model with all the sample\n",
    "# print \"buiding the model with all sample as save with pickl\"\n",
    "att = [\"Fare_amount\", \"Total_amount\", \"Tip_amount\", \"trip_duration\", \"speed_mph\", \"tip_percentage\"]\n",
    "save_optimized_prediction_model(df_sample[att], 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model and predict tip for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('score', 0.98090748740972156)\n",
      "('Mean Squared error', 26.762485045646425)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 28.64      ,  22.5       ,   0.        , ...,  20.9122807 ,\n",
       "        13.79310345,  27.92988148])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model and predict tip for test\n",
    "df_new = df.copy()\n",
    "print \"data cleaning\"\n",
    "df_new = data_cleanining(df_new)\n",
    "print \"feature engineering\"\n",
    "df_new = data_engin(df_new)\n",
    "test = df_new.loc[np.random.choice(df_new.index,size=100000,replace=False)]\n",
    "att = [\"Fare_amount\", \"Total_amount\", \"Tip_amount\", \"trip_duration\", \"speed_mph\", \"tip_percentage\"]\n",
    "predict_tip(test[att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
